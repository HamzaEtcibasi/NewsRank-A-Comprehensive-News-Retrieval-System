{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lupyne\n",
      "  Downloading lupyne-3.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: lupyne\n",
      "Successfully installed lupyne-3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "ERROR: Could not find a version that satisfies the requirement Cyton (from versions: none)\n",
      "ERROR: No matching distribution found for Cyton\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orchvision (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\asus\\anaconda3\\envs\\intel-image-cls\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install lupyne\n",
    "!pip install Cyton"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:41:55.826499400Z",
     "start_time": "2024-06-01T14:41:41.066496600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Assuming the CSV file is named 'wapo.csv'\n",
    "with open('wapo.csv', 'r', newline='', encoding='utf-16') as csvfile:\n",
    "  reader = csv.reader(csvfile, delimiter=',', quotechar='\"')  # Double quotes as delimiters\n",
    "\n",
    "  # Skip the header row (optional)\n",
    "  next(reader)\n",
    "\n",
    "  # List to store extracted data\n",
    "  data = []\n",
    "  for row in reader:\n",
    "    if len(row) >= 2 and \"\\t\\t\\t\" not in row[0]: \n",
    "        # Extract id and title from each row\n",
    "        id = row[0]\n",
    "        title = row[1].strip()  # Remove any leading/trailing whitespaces from title\n",
    "\n",
    "        # Append data to the list\n",
    "        data.append({'id': id, 'title': title})\n",
    "\n",
    "# Now you have a list of dictionaries containing 'id' and 'title'\n",
    "print(len(data))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inverted Index Implementation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import re as re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# removes the suffixes from an English word and obtain its stem\n",
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def tokenize(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    #tokens = text.split()  # Split into tokens\n",
    "    # tokens = [token for token in text.split() if token not in stop_words]\n",
    "    \n",
    "    # keep 1st, 2nd, 3rd, 24st (ordinal numbers) etc but separate words like 10monthold, 10year, 10yearold, 20point, 20game, 23march, 24hour etc.\n",
    "    text = re.sub(r'(?<!\\d)(\\d+)(?!st|nd|rd|th)(?!\\d)', r'\\1 ', text)  # Add space between digits and letters except for ordinal numbers\n",
    "\n",
    "    tokens = [porter_stemmer.stem(token) for token in text.split() if token not in stop_words]\n",
    "\n",
    "    return tokens\n",
    "\n",
    "def build_inverted_index(file_path):\n",
    "    inverted_index = {}\n",
    "    doc_lengths = {}  # Dictionary to store document lengths\n",
    "    doc_titles = {}  # Dictionary to store document titles\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-16') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) >= 2 and \"\\t\\t\\t\" not in row[0]: \n",
    "                doc_id = row[0]  # Assuming doc_id is in the first column\n",
    "                title = row[1]   # Assuming title is in the second column\n",
    "                \n",
    "                # Tokenize title\n",
    "                tokens = tokenize(title)\n",
    "                \n",
    "                # Update document lengths\n",
    "                doc_lengths[doc_id] = len(tokens)\n",
    "                \n",
    "                # Update inverted index\n",
    "                for token in tokens:\n",
    "                    if token not in inverted_index:\n",
    "                        inverted_index[token] = {'doc_freq': 0, 'postings': {}}\n",
    "                    \n",
    "                    if doc_id not in inverted_index[token]['postings']:\n",
    "                        inverted_index[token]['postings'][doc_id] = 0\n",
    "                    \n",
    "                    inverted_index[token]['postings'][doc_id] += 1\n",
    "                    inverted_index[token]['doc_freq'] += 1\n",
    "\n",
    "                # Update document titles\n",
    "                doc_titles[doc_id] = title\n",
    "\n",
    "    # Sort terms alphabetically\n",
    "    sorted_index = {}\n",
    "    for term in sorted(inverted_index):\n",
    "        sorted_index[term] = inverted_index[term]\n",
    "    \n",
    "    return sorted_index, doc_lengths, doc_titles\n",
    "\n",
    "# def main():\n",
    "#     file_path = 'wapo.csv'\n",
    "#     inverted_index, _ = build_inverted_index(file_path)\n",
    "# \n",
    "#     # Print the inverted index\n",
    "#     sorted_terms = sorted(inverted_index.keys())\n",
    "#     for term in sorted_terms[2000:2200]:  # Print only the first 10 terms\n",
    "#         print(f'{term}: {inverted_index[term]}')\n",
    "\n",
    "file_path = 'wapo.csv'\n",
    "inverted_index, doc_lengths, doc_titles = build_inverted_index(file_path)\n",
    "\n",
    "import json\n",
    "\n",
    "# Save the inverted index, document lengths, and document titles to files\n",
    "def save_to_files(inverted_index, doc_lengths, doc_titles):\n",
    "    with open('inverted_index.json', 'w') as f:\n",
    "        json.dump(inverted_index, f)\n",
    "    \n",
    "    with open('doc_lengths.json', 'w') as f:\n",
    "        json.dump(doc_lengths, f)\n",
    "    \n",
    "    with open('doc_titles.json', 'w') as f:\n",
    "        json.dump(doc_titles, f)\n",
    "\n",
    "# Save the data to files\n",
    "save_to_files(inverted_index, doc_lengths, doc_titles)\n",
    "\n",
    "# Write the inverted index to a text file\n",
    "with open('inverted_index.txt', 'w') as f:\n",
    "    sorted_terms = sorted(inverted_index.keys())\n",
    "    for term in sorted_terms[0:5000]: \n",
    "        f.write(f'{term}: {inverted_index[term]}\\n')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Inverted Index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the inverted index, document lengths, and document titles from files\n",
    "def load_from_files():\n",
    "    with open('inverted_index.json', 'r') as f:\n",
    "        inverted_index = json.load(f)\n",
    "    \n",
    "    with open('doc_lengths.json', 'r') as f:\n",
    "        doc_lengths = json.load(f)\n",
    "    \n",
    "    with open('doc_titles.json', 'r') as f:\n",
    "        doc_titles = json.load(f)\n",
    "    \n",
    "    return inverted_index, doc_lengths, doc_titles\n",
    "\n",
    "# Load the data from files\n",
    "inverted_index, doc_lengths, doc_titles = load_from_files()\n",
    "\n",
    "# Verify loaded data\n",
    "print(f\"Loaded inverted index sample: {list(inverted_index.items())[:5]}\")\n",
    "print(f\"Loaded document lengths sample: {list(doc_lengths.items())[:5]}\")\n",
    "print(f\"Loaded document titles sample: {list(doc_titles.items())[:5]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_stats(inverted_index, doc_lengths):\n",
    "    num_documents = len(doc_lengths)\n",
    "    \n",
    "    num_tokens = len(inverted_index)\n",
    "    \n",
    "    total_words = sum(doc_lengths.values())\n",
    "    \n",
    "    return num_documents, num_tokens, total_words\n",
    "\n",
    "def calculate_average_words_per_document(doc_lengths):\n",
    "    total_words = sum(doc_lengths.values())\n",
    "    total_documents = len(doc_lengths)\n",
    "    average_words = total_words / total_documents if total_documents > 0 else 0\n",
    "    return average_words\n",
    "\n",
    "average_words = calculate_average_words_per_document(doc_lengths)\n",
    "\n",
    "print(f'Average number of words per document: {average_words}')\n",
    "\n",
    "num_documents, num_tokens, total_words = calculate_stats(inverted_index, doc_lengths)\n",
    "print(f\"Number of documents: {num_documents}\")\n",
    "print(f\"Number of tokens: {num_tokens}\")\n",
    "print(f\"Total words: {total_words}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocess Queries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_queries(query_file):\n",
    "    with open(query_file, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # Split the content into individual queries\n",
    "    queries = content.split('<top>')\n",
    "    preprocessed_queries = []\n",
    "\n",
    "    for query in queries[1:]:  # Skip the first empty string\n",
    "        query_parts = query.strip().split('\\n')\n",
    "\n",
    "        # Find the <desc> and </desc> tags and extract the text in between\n",
    "        desc_start = None\n",
    "        desc_end = None\n",
    "        for i, part in enumerate(query_parts):\n",
    "            if part.startswith('<desc> Description:'):\n",
    "                desc_start = i + 1  # Start from the next line after <desc> tag\n",
    "            elif part.startswith('</desc>'):\n",
    "                desc_end = i  # End at the line before </desc> tag\n",
    "                break\n",
    "\n",
    "        if desc_start is not None and desc_end is not None:\n",
    "            query_num = re.search(r'Number: (\\d+)', query_parts[0]).group(1)\n",
    "            query_text = ' '.join([part.strip() for part in query_parts[desc_start:desc_end]])\n",
    "            tokens = tokenize(query_text)\n",
    "            preprocessed_queries.append((query_num, ' '.join(tokens)))\n",
    "\n",
    "    return preprocessed_queries\n",
    "\n",
    "# Preprocess the queries\n",
    "query_file = 'topics2018.txt'\n",
    "preprocessed_queries = preprocess_queries(query_file)\n",
    "\n",
    "# Print the preprocessed queries\n",
    "for query_num, preprocessed_query in preprocessed_queries:\n",
    "    print(f\"Query {query_num}: {preprocessed_query}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BM25"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class BM25:\n",
    "    def __init__(self, inverted_index, doc_lengths, k1=1.5, b=0.75):\n",
    "        self.inverted_index = inverted_index\n",
    "        self.doc_lengths = doc_lengths\n",
    "        self.avg_doc_length = sum(doc_lengths.values()) / len(doc_lengths)\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        self.N = len(doc_lengths)\n",
    "        self.idf_cache = {}\n",
    "\n",
    "    def idf(self, term):\n",
    "        if term not in self.idf_cache:\n",
    "            df = self.inverted_index.get(term, {'doc_freq': 0})['doc_freq']\n",
    "            self.idf_cache[term] = math.log((self.N - df + 0.5) / (df + 0.5) + 1)\n",
    "        return self.idf_cache[term]\n",
    "\n",
    "    def score(self, query_terms, doc_id):\n",
    "        score = 0\n",
    "        doc_length = self.doc_lengths[doc_id]\n",
    "        for term in query_terms:\n",
    "            tf = self.inverted_index.get(term, {'postings': {}})['postings'].get(doc_id, 0)\n",
    "            idf = self.idf(term)\n",
    "            score += idf * (tf * (self.k1 + 1)) / (tf + self.k1 * (1 - self.b + self.b * doc_length / self.avg_doc_length))\n",
    "        return score\n",
    "\n",
    "def evaluate_queries(queries, relevant_docs, inverted_index, doc_lengths, k=10):\n",
    "    bm25 = BM25(inverted_index, doc_lengths)\n",
    "    average_precision = 0\n",
    "    precision_at_10 = 0\n",
    "    ndcg_at_10 = 0\n",
    "    total_queries = len(queries)\n",
    "\n",
    "    for query_num, query_text in queries:\n",
    "        query_terms = query_text.split()\n",
    "        scores = [(doc_id, bm25.score(query_terms, doc_id)) for doc_id in doc_lengths.keys()]\n",
    "        ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        relevant_docs_for_query = relevant_docs.get(query_num, [])\n",
    "        precision_count = sum(1 for doc_id, _ in ranked_docs[:k] if doc_id in relevant_docs_for_query)\n",
    "        precision_at_10 += precision_count / k\n",
    "\n",
    "        average_precision += compute_average_precision(ranked_docs, relevant_docs_for_query)\n",
    "        ndcg_at_10 += compute_ndcg_at_k(ranked_docs, relevant_docs_for_query, k)\n",
    "\n",
    "    map_score = average_precision / total_queries\n",
    "    precision_at_10_score = precision_at_10 / total_queries\n",
    "    ndcg_at_10_score = ndcg_at_10 / total_queries\n",
    "\n",
    "    return map_score, precision_at_10_score, ndcg_at_10_score\n",
    "\n",
    "def compute_average_precision(ranked_docs, relevant_docs):\n",
    "    num_relevant_docs = len(relevant_docs)\n",
    "    if num_relevant_docs == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision_sum = 0\n",
    "    num_retrieved_relevant_docs = 0\n",
    "    for i, (doc_id, _) in enumerate(ranked_docs, start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            num_retrieved_relevant_docs += 1\n",
    "            precision_sum += num_retrieved_relevant_docs / i\n",
    "    \n",
    "    return precision_sum / num_relevant_docs\n",
    "\n",
    "def compute_ndcg_at_k(ranked_docs, relevant_docs, k):\n",
    "    if not relevant_docs:\n",
    "        return 0\n",
    "    \n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    # for i, (doc_id, _) in enumerate(ranked_docs[:k], start=1):\n",
    "        # relevance = relevant_docs.get(doc_id, 0)\n",
    "        # gain = (relevance) / math.log2(i + 1)\n",
    "        # dcg += gain\n",
    "        # idcg_gain = (max(relevant_docs.values())) / math.log2(i + 1)\n",
    "        # idcg += idcg_gain\n",
    "    \n",
    "    for i, (doc_id, _) in enumerate(ranked_docs[:k], start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            dcg += 1 / math.log2(i + 1)\n",
    "        idcg += 1 / math.log2(i + 1)\n",
    "\n",
    "    return dcg / idcg\n",
    "\n",
    "def load_relevant_docs(relevant_docs_file):\n",
    "    relevant_docs = {}\n",
    "    with open(relevant_docs_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            query_num = parts[0]\n",
    "            doc_id = parts[2]\n",
    "            relevance = int(parts[3])  # 0 for non-relevant, 1 for relevant, 2 for highly relevant\n",
    "            if relevance >= 0:  # considering all relevance levels\n",
    "                if query_num in relevant_docs:\n",
    "                    relevant_docs[query_num][doc_id] = relevance\n",
    "                else:\n",
    "                    relevant_docs[query_num] = {doc_id: relevance}\n",
    "    return relevant_docs\n",
    "\n",
    "\n",
    "query_file = 'topics2018.txt'\n",
    "queries = preprocess_queries(query_file)\n",
    "\n",
    "relevant_docs_file = 'qrels2018.txt'  # File containing ground truth relevance judgments\n",
    "relevant_docs = load_relevant_docs(relevant_docs_file)\n",
    "\n",
    "map_score, precision_at_10_score, ndcg_at_10_score = evaluate_queries(queries, relevant_docs, inverted_index, doc_lengths)\n",
    "\n",
    "print(f'Mean Average Precision (MAP): {map_score}')\n",
    "print(f'Precision@10: {precision_at_10_score}')\n",
    "print(f'NDCG@10: {ndcg_at_10_score}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test BM25"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_bm25_to_query(query, inverted_index, doc_lengths, k=20):\n",
    "    bm25 = BM25(inverted_index, doc_lengths)\n",
    "    query_terms = tokenize(query)\n",
    "    scores = [(doc_id, bm25.score(query_terms, doc_id)) for doc_id in doc_lengths.keys()]\n",
    "    ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "    return ranked_docs\n",
    "\n",
    "# Example usage:\n",
    "query = \"How does climate change affect weather patterns?\"\n",
    "query = \"Map shows changes to climate\"\n",
    "ranked_docs = apply_bm25_to_query(query, inverted_index, doc_lengths)\n",
    "print(\"Ranked Documents:\")\n",
    "for rank, (doc_id, score) in enumerate(ranked_docs, start=1):\n",
    "    print(f\"{rank}. Document ID: {doc_id}, Score: {score}, {doc_titles[doc_id]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Query Likelihood"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "\"\"\n",
    "class QueryLikelihoodModel:\n",
    "    def __init__(self, inverted_index, doc_lengths, smoothing_param=0.5):\n",
    "        self.inverted_index = inverted_index\n",
    "        self.doc_lengths = doc_lengths\n",
    "        self.smoothing_param = smoothing_param\n",
    "        self.total_terms = sum(doc_lengths.values())\n",
    "        self.term_freq_cache = {}\n",
    "\n",
    "    def term_freq(self, term, doc_id):\n",
    "        if (term, doc_id) not in self.term_freq_cache:\n",
    "            tf = self.inverted_index.get(term, {'postings': {}})['postings'].get(doc_id, 0)\n",
    "            doc_length = self.doc_lengths[doc_id]\n",
    "            self.term_freq_cache[(term, doc_id)] = (tf + self.smoothing_param) / (doc_length + self.smoothing_param * len(self.inverted_index))\n",
    "        return self.term_freq_cache[(term, doc_id)]\n",
    "\n",
    "    def coll_freq(self, term):\n",
    "        return self.inverted_index.get(term, {'doc_freq': 0})['doc_freq']\n",
    "\n",
    "    def score(self, query_terms, doc_id):\n",
    "        score = 0\n",
    "        for term in query_terms:\n",
    "            term_freq = self.term_freq(term, doc_id)\n",
    "            coll_freq = self.coll_freq(term) / self.total_terms\n",
    "            \n",
    "            # Add a small positive value (e.g., 1e-10) to prevent log(0)\n",
    "            score += math.log(term_freq + 1e-10) - math.log(coll_freq + 1e-10)\n",
    "        return score\n",
    "\n",
    "def compute_average_precision(ranked_docs, relevant_docs):\n",
    "    num_relevant_docs = len(relevant_docs)\n",
    "    if num_relevant_docs == 0:\n",
    "        return 0\n",
    "    \n",
    "    precision_sum = 0\n",
    "    num_retrieved_relevant_docs = 0\n",
    "    for i, (doc_id, _) in enumerate(ranked_docs, start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            num_retrieved_relevant_docs += 1\n",
    "            precision_sum += num_retrieved_relevant_docs / i\n",
    "    \n",
    "    return precision_sum / num_relevant_docs\n",
    "\n",
    "def compute_ndcg_at_k(ranked_docs, relevant_docs, k):\n",
    "    if not relevant_docs:\n",
    "        return 0\n",
    "    \n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "    # for i, (doc_id, _) in enumerate(ranked_docs[:k], start=1):\n",
    "        # relevance = relevant_docs.get(doc_id, 0)\n",
    "        # gain = (relevance) / math.log2(i + 1)\n",
    "        # dcg += gain\n",
    "        # idcg_gain = (max(relevant_docs.values())) / math.log2(i + 1)\n",
    "        # idcg += idcg_gain\n",
    "    \n",
    "    for i, (doc_id, _) in enumerate(ranked_docs[:k], start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            dcg += 1 / math.log2(i + 1)\n",
    "        idcg += 1 / math.log2(i + 1)\n",
    "\n",
    "    return dcg / idcg\n",
    "\n",
    "def load_relevant_docs(relevant_docs_file):\n",
    "    relevant_docs = {}\n",
    "    with open(relevant_docs_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            query_num = parts[0]\n",
    "            doc_id = parts[2]\n",
    "            relevance = int(parts[3])  # 0 for non-relevant, 1 for relevant, 2 for highly relevant\n",
    "            if relevance >= 0:  # considering all relevance levels\n",
    "                if query_num in relevant_docs:\n",
    "                    relevant_docs[query_num][doc_id] = relevance\n",
    "                else:\n",
    "                    relevant_docs[query_num] = {doc_id: relevance}\n",
    "    return relevant_docs\n",
    "\n",
    "def evaluate_queries_ql(queries, relevant_docs, inverted_index, doc_lengths, k=10):\n",
    "    qln_model = QueryLikelihoodModel(inverted_index, doc_lengths)\n",
    "    average_precision = 0\n",
    "    precision_at_10 = 0\n",
    "    ndcg_at_10 = 0\n",
    "    total_queries = len(queries)\n",
    "\n",
    "    for query_num, query_text in queries:\n",
    "        query_terms = query_text.split()\n",
    "        scores = [(doc_id, qln_model.score(query_terms, doc_id)) for doc_id in doc_lengths.keys()]\n",
    "        ranked_docs = sorted(scores, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        relevant_docs_for_query = relevant_docs.get(query_num, [])\n",
    "        precision_count = sum(1 for doc_id, _ in ranked_docs[:k] if doc_id in relevant_docs_for_query)\n",
    "        precision_at_10 += precision_count / k\n",
    "\n",
    "        average_precision += compute_average_precision(ranked_docs, relevant_docs_for_query)\n",
    "        ndcg_at_10 += compute_ndcg_at_k(ranked_docs, relevant_docs_for_query, k)\n",
    "\n",
    "    map_score = average_precision / total_queries\n",
    "    precision_at_10_score = precision_at_10 / total_queries\n",
    "    ndcg_at_10_score = ndcg_at_10 / total_queries\n",
    "\n",
    "    return map_score, precision_at_10_score, ndcg_at_10_score\n",
    "\n",
    "\n",
    "query_file = 'topics2018.txt'\n",
    "queries = preprocess_queries(query_file)\n",
    "\n",
    "relevant_docs_file = 'qrels2018.txt'  # File containing ground truth relevance judgments\n",
    "relevant_docs = load_relevant_docs(relevant_docs_file)\n",
    "\n",
    "map_score, precision_at_10_score, ndcg_at_10_score = evaluate_queries_ql(queries, relevant_docs, inverted_index, doc_lengths)\n",
    "    \n",
    "print(f'Mean Average Precision (MAP): {map_score}')\n",
    "print(f'Precision@10: {precision_at_10_score}')\n",
    "print(f'NDCG@10: {ndcg_at_10_score}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Phrase Query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def build_inverted_index_with_position(file_path):\n",
    "    inverted_index = {}\n",
    "    doc_lengths = {}  # Dictionary to store document lengths\n",
    "    doc_titles = {}  # Dictionary to store document titles\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-16') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if len(row) >= 2 and \"\\t\\t\\t\" not in row[0]: \n",
    "                doc_id = row[0]  # Assuming doc_id is in the first column\n",
    "                title = row[1]   # Assuming title is in the second column\n",
    "                \n",
    "                # Tokenize title\n",
    "                tokens = tokenize(title)\n",
    "                \n",
    "                # Update document lengths\n",
    "                doc_lengths[doc_id] = len(tokens)\n",
    "                \n",
    "                # Update inverted index with positional information\n",
    "                for pos, token in enumerate(tokens):\n",
    "                    if token not in inverted_index:\n",
    "                        inverted_index[token] = {'doc_freq': 0, 'postings': {}}\n",
    "                    \n",
    "                    if doc_id not in inverted_index[token]['postings']:\n",
    "                        inverted_index[token]['postings'][doc_id] = []\n",
    "                    \n",
    "                    inverted_index[token]['postings'][doc_id].append(pos)\n",
    "                    inverted_index[token]['doc_freq'] += 1\n",
    "\n",
    "                # Update document titles\n",
    "                doc_titles[doc_id] = title\n",
    "\n",
    "    # Sort terms alphabetically\n",
    "    sorted_index = {}\n",
    "    for term in sorted(inverted_index):\n",
    "        sorted_index[term] = inverted_index[term]\n",
    "    \n",
    "    return sorted_index, doc_lengths, doc_titles\n",
    "\n",
    "def phrase_query(inverted_index, query):\n",
    "    query_terms = tokenize(query)\n",
    "    doc_scores = {}\n",
    "    \n",
    "    # Initialize document scores\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            for doc_id, positions in inverted_index[term]['postings'].items():\n",
    "                if doc_id not in doc_scores:\n",
    "                    doc_scores[doc_id] = 0\n",
    "    \n",
    "    # Calculate scores based on phrase proximity\n",
    "    for term in query_terms:\n",
    "        if term in inverted_index:\n",
    "            for doc_id, positions in inverted_index[term]['postings'].items():\n",
    "                if all(doc_id in inverted_index[qt]['postings'] for qt in query_terms):\n",
    "                    for pos in positions:\n",
    "                        match_count = 0\n",
    "                        for qt in query_terms:\n",
    "                            if pos + query_terms.index(qt) in inverted_index[qt]['postings'][doc_id]:\n",
    "                                match_count += 1\n",
    "                        if match_count == len(query_terms):\n",
    "                            doc_scores[doc_id] += 1\n",
    "    \n",
    "    # Sort documents by score\n",
    "    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return sorted_docs\n",
    "\n",
    "# Example usage:\n",
    "file_path = 'wapo.csv'\n",
    "inverted_index_pq, _, doc_titles_pq = build_inverted_index_pq(file_path)\n",
    "query = \"As homicides\"\n",
    "query = \"Danny Coale\"\n",
    "\n",
    "results = phrase_query(inverted_index_pq, query)\n",
    "\n",
    "# Print top 10 results\n",
    "for doc_id, score in results[:10]:\n",
    "    if score == 1:\n",
    "        print(f\"Document ID: {doc_id}, Score: {score}, {doc_titles_pq[doc_id]}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Boolean Query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_boolean_query(input):\n",
    "    \"\"\"\n",
    "    Parse a boolean query string and extract terms and operation.\n",
    "\n",
    "    :param input: Boolean query string\n",
    "    :return: Two terms extracted from the query string and the operation\n",
    "    \"\"\"\n",
    "    # Split the input string based on the operation keywords\n",
    "    if \"AND\" in input:\n",
    "        operation = \"AND\"\n",
    "        terms = input.split(\"AND\")\n",
    "    elif \"OR\" in input:\n",
    "        operation = \"OR\"\n",
    "        terms = input.split(\"OR\")\n",
    "    elif \"NOT\" in input:\n",
    "        operation = \"NOT\"\n",
    "        terms = input.split(\"NOT\")\n",
    "    \n",
    "    # Strip leading and trailing spaces from each term and the operation\n",
    "    term1 = terms[0].strip()\n",
    "    term2 = terms[1].strip()\n",
    "    \n",
    "    return term1, term2, operation\n",
    "\n",
    "def get_term_postings_set(inverted_index, term):\n",
    "    if term in inverted_index:\n",
    "        return set(inverted_index[term]['postings'].keys())\n",
    "    else:\n",
    "        # If the term is not in the index, return an empty set\n",
    "        return set()\n",
    "\n",
    "def process_and_boolean_query(inverted_index, term1, term2):\n",
    "    # Retrieve sets for each term\n",
    "    term1_postings = get_term_postings_set(inverted_index, term1)\n",
    "    term2_postings = get_term_postings_set(inverted_index, term2)\n",
    "    \n",
    "    # Find the intersection of the two sets\n",
    "    intersection = term1_postings.intersection(term2_postings)\n",
    "    \n",
    "    return term1_postings, term2_postings, intersection\n",
    "\n",
    "def process_or_boolean_query(inverted_index, term1, term2):\n",
    "    # Retrieve sets for each term\n",
    "    term1_postings = get_term_postings_set(inverted_index, term1)\n",
    "    term2_postings = get_term_postings_set(inverted_index, term2)\n",
    "    \n",
    "    # Find the union of the two sets\n",
    "    union = term1_postings.union(term2_postings)\n",
    "    \n",
    "    return term1_postings, term2_postings, union\n",
    "\n",
    "def process_not_boolean_query(inverted_index, term):\n",
    "    # Retrieve the set of document IDs for the given term\n",
    "    term_postings = get_term_postings_set(inverted_index, term)\n",
    "    \n",
    "    # Retrieve all document IDs in the index\n",
    "    all_doc_ids = set()\n",
    "    for postings in inverted_index.values():\n",
    "        all_doc_ids.update(postings['postings'].keys())\n",
    "    \n",
    "    # Find the set of document IDs that do not contain the term\n",
    "    not_term_postings = all_doc_ids - term_postings\n",
    "    \n",
    "    return not_term_postings\n",
    "\n",
    "def process_boolean_query(query):\n",
    "    parsed_term1, parsed_term2, operation = parse_boolean_query(query)\n",
    "    if(operation == \"AND\"):\n",
    "        term1 = tokenize(parsed_term1)[0]\n",
    "        term2 = tokenize(parsed_term2)[0]\n",
    "        term1_postings, term2_postings, intersection = process_and_boolean_query(inverted_index, term1, term2)\n",
    "        return intersection\n",
    "\n",
    "    elif operation == \"OR\":\n",
    "        term1 = tokenize(parsed_term1)[0]\n",
    "        term2 = tokenize(parsed_term2)[0]\n",
    "        term1_postings, term2_postings, union = process_or_boolean_query(inverted_index, term1, term2)\n",
    "        return union\n",
    "\n",
    "    elif operation == \"NOT\":\n",
    "        term = tokenize(parsed_term2)[0]\n",
    "        not_term_postings = process_not_boolean_query(inverted_index, term)\n",
    "        return not_term_postings\n",
    "\n",
    "\n",
    "query = \"Maryland AND win AND blair\"\n",
    "results = process_boolean_query(query)\n",
    "\n",
    "counter = 0\n",
    "for doc_id in results:\n",
    "    if counter >= 10:\n",
    "        break\n",
    "    title = doc_titles.get(doc_id, \"Title not available\")\n",
    "    print(f\"Document ID: {doc_id} {title}\")  \n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extended Boolean Query"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import collections\n",
    "\n",
    "# Load inverted index and document titles from files\n",
    "with open('inverted_index.json', 'r') as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "with open('doc_titles.json', 'r') as f:\n",
    "    doc_titles = json.load(f)\n",
    "\n",
    "def process_query(query, inverted_index, doc_titles):\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()  # Use the PorterStemmer for consistency with your indexing\n",
    "    indexed_docIDs = sorted(doc_titles.keys())  # All docIDs from your titles\n",
    "    \n",
    "    # Prepare query list\n",
    "    query = query.replace('(', '( ')\n",
    "    query = query.replace(')', ' )')\n",
    "    query = query.split(' ')\n",
    "    \n",
    "    results_stack = []\n",
    "    postfix_queue = collections.deque(shunting_yard(query))  # Convert infix to postfix\n",
    "    \n",
    "    while postfix_queue:\n",
    "        token = postfix_queue.popleft()\n",
    "        result = []\n",
    "        \n",
    "        if token not in ['AND', 'OR', 'NOT']:\n",
    "            token = stemmer.stem(token)\n",
    "            if token in inverted_index:\n",
    "                result = list(inverted_index[token]['postings'].keys())\n",
    "        elif token == 'AND':\n",
    "            right_operand = results_stack.pop()\n",
    "            left_operand = results_stack.pop()\n",
    "            result = boolean_AND(left_operand, right_operand, indexed_docIDs)\n",
    "        elif token == 'OR':\n",
    "            right_operand = results_stack.pop()\n",
    "            left_operand = results_stack.pop()\n",
    "            result = boolean_OR(left_operand, right_operand, indexed_docIDs)\n",
    "        elif token == 'NOT':\n",
    "            right_operand = results_stack.pop()\n",
    "            result = boolean_NOT(right_operand, indexed_docIDs)\n",
    "        \n",
    "        results_stack.append(result)\n",
    "    \n",
    "    return results_stack.pop()\n",
    "\n",
    "def shunting_yard(infix_tokens):\n",
    "    precedence = {'NOT': 3, 'AND': 2, 'OR': 1, '(': 0, ')': 0}\n",
    "    output = []\n",
    "    operator_stack = []\n",
    "    \n",
    "    for token in infix_tokens:\n",
    "        if token == '(':\n",
    "            operator_stack.append(token)\n",
    "        elif token == ')':\n",
    "            while operator_stack and operator_stack[-1] != '(':\n",
    "                output.append(operator_stack.pop())\n",
    "            operator_stack.pop()  # Pop '('\n",
    "        elif token in precedence:\n",
    "            while operator_stack and precedence[operator_stack[-1]] >= precedence[token]:\n",
    "                output.append(operator_stack.pop())\n",
    "            operator_stack.append(token)\n",
    "        else:\n",
    "            output.append(token)\n",
    "    \n",
    "    while operator_stack:\n",
    "        output.append(operator_stack.pop())\n",
    "    \n",
    "    return output\n",
    "\n",
    "def boolean_AND(left_operand, right_operand, indexed_docIDs):\n",
    "    left_set = set(left_operand)\n",
    "    right_set = set(right_operand)\n",
    "    return list(left_set.intersection(right_set))\n",
    "\n",
    "def boolean_OR(left_operand, right_operand, indexed_docIDs):\n",
    "    left_set = set(left_operand)\n",
    "    right_set = set(right_operand)\n",
    "    return list(left_set.union(right_set))\n",
    "\n",
    "def boolean_NOT(right_operand, indexed_docIDs):\n",
    "    return list(set(indexed_docIDs) - set(right_operand))\n",
    "\n",
    "# Example query\n",
    "query = \"Maryland AND (win OR NOT financial) AND Bowie\"\n",
    "query = \"Maryland AND (win OR NOT financial) AND Bowie AND NOT overmatched\"\n",
    "query = \"Maryland AND (Baseball OR Ivey) AND Bowie AND NOT overmatched\"\n",
    "\n",
    "query = \"NOT Eleanor AND (win OR NOT financial) AND (Maryland AND Bowie AND NOT Sherwood)\"\n",
    "query = \"Ivey AND Brown\"\n",
    "query = \"Ivey AND Brown AND NOT 2014\"\n",
    "results = process_query(query, inverted_index, doc_titles)\n",
    "\n",
    "# Print first 10 results\n",
    "counter = 0\n",
    "for doc_id in results:\n",
    "    if counter >= 10:\n",
    "        break\n",
    "    title = doc_titles.get(doc_id, \"Title not available\")\n",
    "    print(f\"Document ID: {doc_id} Title: {title}\")\n",
    "    counter += 1\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18172\\1354220126.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    120\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    121\u001B[0m \u001B[1;31m# Run the application\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 122\u001B[1;33m \u001B[0mroot\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmainloop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    123\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\envs\\intel-image-cls\\lib\\tkinter\\__init__.py\u001B[0m in \u001B[0;36mmainloop\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m   1281\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mmainloop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1282\u001B[0m         \u001B[1;34m\"\"\"Call the mainloop of Tk.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1283\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmainloop\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mn\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1284\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mquit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1285\u001B[0m         \u001B[1;34m\"\"\"Quit the Tcl interpreter. All widgets will be destroyed.\"\"\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import collections\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "# Load inverted index and document titles from files\n",
    "with open('inverted_index.json', 'r') as f:\n",
    "    inverted_index = json.load(f)\n",
    "\n",
    "with open('doc_titles.json', 'r') as f:\n",
    "    doc_titles = json.load(f)\n",
    "\n",
    "def process_query(query, inverted_index, doc_titles):\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()  # Use the PorterStemmer for consistency with your indexing\n",
    "    indexed_docIDs = sorted(doc_titles.keys())  # All docIDs from your titles\n",
    "    \n",
    "    # Prepare query list\n",
    "    query = query.replace('(', '( ')\n",
    "    query = query.replace(')', ' )')\n",
    "    query = query.split(' ')\n",
    "    \n",
    "    results_stack = []\n",
    "    postfix_queue = collections.deque(shunting_yard(query))  # Convert infix to postfix\n",
    "    \n",
    "    while postfix_queue:\n",
    "        token = postfix_queue.popleft()\n",
    "        result = []\n",
    "        \n",
    "        if token not in ['AND', 'OR', 'NOT']:\n",
    "            token = stemmer.stem(token)\n",
    "            if token in inverted_index:\n",
    "                result = list(inverted_index[token]['postings'].keys())\n",
    "        elif token == 'AND':\n",
    "            right_operand = results_stack.pop()\n",
    "            left_operand = results_stack.pop()\n",
    "            result = boolean_AND(left_operand, right_operand, indexed_docIDs)\n",
    "        elif token == 'OR':\n",
    "            right_operand = results_stack.pop()\n",
    "            left_operand = results_stack.pop()\n",
    "            result = boolean_OR(left_operand, right_operand, indexed_docIDs)\n",
    "        elif token == 'NOT':\n",
    "            right_operand = results_stack.pop()\n",
    "            result = boolean_NOT(right_operand, indexed_docIDs)\n",
    "        \n",
    "        results_stack.append(result)\n",
    "    \n",
    "    return results_stack.pop()\n",
    "\n",
    "def shunting_yard(infix_tokens):\n",
    "    precedence = {'NOT': 3, 'AND': 2, 'OR': 1, '(': 0, ')': 0}\n",
    "    output = []\n",
    "    operator_stack = []\n",
    "    \n",
    "    for token in infix_tokens:\n",
    "        if token == '(':\n",
    "            operator_stack.append(token)\n",
    "        elif token == ')':\n",
    "            while operator_stack and operator_stack[-1] != '(':\n",
    "                output.append(operator_stack.pop())\n",
    "            operator_stack.pop()  # Pop '('\n",
    "        elif token in precedence:\n",
    "            while operator_stack and precedence[operator_stack[-1]] >= precedence[token]:\n",
    "                output.append(operator_stack.pop())\n",
    "            operator_stack.append(token)\n",
    "        else:\n",
    "            output.append(token)\n",
    "    \n",
    "    while operator_stack:\n",
    "        output.append(operator_stack.pop())\n",
    "    \n",
    "    return output\n",
    "\n",
    "def boolean_AND(left_operand, right_operand, indexed_docIDs):\n",
    "    left_set = set(left_operand)\n",
    "    right_set = set(right_operand)\n",
    "    return list(left_set.intersection(right_set))\n",
    "\n",
    "def boolean_OR(left_operand, right_operand, indexed_docIDs):\n",
    "    left_set = set(left_operand)\n",
    "    right_set = set(right_operand)\n",
    "    return list(left_set.union(right_set))\n",
    "\n",
    "def boolean_NOT(right_operand, indexed_docIDs):\n",
    "    return list(set(indexed_docIDs) - set(right_operand))\n",
    "\n",
    "def search_query():\n",
    "    query = query_entry.get()\n",
    "    results = process_query(query, inverted_index, doc_titles)\n",
    "    results_list.delete(*results_list.get_children())\n",
    "    counter = 0\n",
    "    for doc_id in results:\n",
    "        if counter >= 10:\n",
    "            break\n",
    "        title = doc_titles.get(doc_id, \"Title not available\")\n",
    "        results_list.insert(\"\", \"end\", values=(doc_id, title))\n",
    "        counter += 1\n",
    "\n",
    "# Create the main window\n",
    "root = tk.Tk()\n",
    "root.title(\"Boolean Query Search\")\n",
    "root.geometry(\"600x400\")\n",
    "\n",
    "# Create and place the query input field\n",
    "query_label = tk.Label(root, text=\"Enter Query:\")\n",
    "query_label.pack(pady=10)\n",
    "query_entry = tk.Entry(root, width=80)\n",
    "query_entry.pack(pady=5)\n",
    "\n",
    "# Create and place the search button\n",
    "search_button = tk.Button(root, text=\"Search\", command=search_query)\n",
    "search_button.pack(pady=5)\n",
    "\n",
    "# Create and place the results treeview\n",
    "columns = (\"Document ID\", \"Document Title\")\n",
    "results_list = ttk.Treeview(root, columns=columns, show=\"headings\")\n",
    "results_list.heading(\"Document ID\", text=\"Document ID\")\n",
    "results_list.heading(\"Document Title\", text=\"Document Title\")\n",
    "results_list.pack(pady=20, fill=tk.BOTH, expand=True)\n",
    "\n",
    "# Run the application\n",
    "root.mainloop()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-01T14:44:37.286204800Z",
     "start_time": "2024-06-01T14:42:29.581574100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "source_hash": "b623e53d",
    "deepnote_to_be_reexecuted": true,
    "cell_id": "2bfa83dd925d46dea9f4fde9ef1fd184",
    "deepnote_cell_type": "code"
   },
   "source": [],
   "block_group": "30ebe94284ca475dbdac45d8e99eea29",
   "execution_count": null,
   "outputs": [],
   "outputs_reference": null,
   "content_dependencies": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   }
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "deepnote_notebook_id": "146d86618dfb440794b4624d75ef7c56",
  "deepnote_execution_queue": [],
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 }
}
