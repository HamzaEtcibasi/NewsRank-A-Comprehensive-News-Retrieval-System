import numpy as np

def parse_output_file(filepath):
    data = {}
    with open(filepath, 'r') as f:
        for line in f:
            parts = line.strip().split('\t')
            topic = int(parts[0])
            doc_id = parts[2]
            rank = int(parts[3])
            score = float(parts[4])
            if topic not in data:
                data[topic] = []
            data[topic].append((rank, doc_id, score))
    return data

def compute_ndcg(relevance_scores, k=10):
    dcg = 0.0
    for i, rel in enumerate(relevance_scores[:k]):
        dcg += (2**rel - 1) / np.log2(i + 2)
    idcg = 0.0
    sorted_rels = sorted(relevance_scores, reverse=True)[:k]
    for i, rel in enumerate(sorted_rels):
        idcg += (2**rel - 1) / np.log2(i + 2)
    return dcg / idcg if idcg > 0 else 0.0

def compute_precision(relevance_scores, k=10):
    return sum(relevance_scores[:k]) / k

def compute_map(relevance_scores):
    ap_sum = 0.0
    num_rel = 0
    for i, rel in enumerate(relevance_scores):
        if rel > 0:
            num_rel += 1
            ap_sum += num_rel / (i + 1)
    return ap_sum / num_rel if num_rel > 0 else 0.0

def calculate_metrics(data, relevance_dict, k=10):
    ndcg_scores = []
    precision_scores = []
    map_scores = []
    for topic, results in data.items():
        results.sort()
        relevance_scores = [relevance_dict[topic][doc_id] for _, doc_id, _ in results]
        ndcg_scores.append(compute_ndcg(relevance_scores, k))
        precision_scores.append(compute_precision(relevance_scores, k))
        map_scores.append(compute_map(relevance_scores))
    return np.mean(ndcg_scores), np.mean(precision_scores), np.mean(map_scores)

def parse_qrels_file(filepath):
    relevance_dict = {}
    with open(filepath, 'r') as f:
        for line in f:
            parts = line.strip().split()
            topic = int(parts[0])
            doc_id = parts[2]
            relevance = int(parts[3])
            if topic not in relevance_dict:
                relevance_dict[topic] = {}
            relevance_dict[topic][doc_id] = relevance
    return relevance_dict

output_file = 'first_run_output'  # The file generated by your evaluation script
qrels_file = '../data/qrels_test.txt'  # The file with the true relevance scores

# Parse the files
data = parse_output_file(output_file)
relevance_dict = parse_qrels_file(qrels_file)

# Calculate NDCG@10, Precision@10, and MAP
ndcg_10, precision_10, map_score = calculate_metrics(data, relevance_dict, k=10)

print(f'NDCG@10: {ndcg_10}')
print(f'Precision@10: {precision_10}')
print(f'MAP: {map_score}')
